# -*- coding: utf-8 -*-
"""nlp_bertTweet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WoSjhg3FuyYq4DORQGPzmSi-l3iHtc2N
"""

!pip install json
!pip install spacy[transformers]
!pip install emoji
!pip install contractions
!pip install textblob
!pip install torch
!pip install transformers

from google.colab import drive
drive.mount('/content/drive')

import json
import emoji
import re, string, unicodedata
import nltk
import contractions
import inflect
from nltk import word_tokenize
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from transformers import AutoModel, AutoTokenizer 
import pandas as pd
import numpy as np
import nltk
import torch
from scipy.spatial.distance import cosine
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt

"""**TRAIN**"""

def load_instances(fn):
    instances = []
    with open(fn) as f:
        for jl_str in f:
            instances.append(json.loads(jl_str))
    
    return instances

trial_instances = load_instances('/content/drive/MyDrive/TempoWiC_Starting_Kit/data/train.data.jl')

def convert(s):
    new = ""
    for x in s:
        new += x
    return new

def replace_contractions(text):
    return contractions.fix(text)

def remove_URL(sample):
    return re.sub(r"http\S+", "", sample)

def remove_non_ascii(words):
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    temp=convert(new_words)
    return temp

def to_lowercase(words):
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return convert(new_words)

def remove_punctuation(words):
    new_words = []
    for word in words:
        new_word = re.sub(r'[^\w\s]', ' ', word)
        if new_word != '':
            new_words.append(new_word)
    return convert(new_words)

def replace_numbers(words):
    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return convert(new_words)

texts1=[]
texts2=[]
words=[]
id=[]
for inst in trial_instances:
  t1= " ".join(emoji.demojize(inst['tweet1']['text']).replace(","," ").replace(":"," ").replace("_"," ").split())
  t1=replace_contractions(t1)
  t1=remove_URL(t1)
  t1=remove_non_ascii(t1)
  t1=remove_punctuation(t1)
  t1=replace_numbers(t1)
  t1=to_lowercase(t1)
  texts1.append(t1)

  t2=" ".join(emoji.demojize(inst['tweet2']['text']).replace(","," ").replace(":"," ").replace("_"," ").split())
  t2=replace_contractions(t2)
  t2=remove_URL(t2)
  t2=remove_non_ascii(t2)
  t2=remove_punctuation(t2)
  t2=replace_numbers(t2)
  t2=to_lowercase(t2)
  texts2.append(t2) 
  
  words.append(inst['word'])
  id.append(inst['id'])

model = AutoModel.from_pretrained("vinai/bertweet-base",output_hidden_states = True)
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base",use_fast=True)

for word in words:
  tokenizer.add_tokens(word)
  
model.resize_token_embeddings(len(tokenizer))

def berttweet_text_preparation(text, tokenizer):
    marked_text = "<s>" + text + "</s>"
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [1]*len(indexed_tokens)
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    return tokenized_text, tokens_tensor, segments_tensors

def get_berttweet_embeddings(tokens_tensor, segments_tensors, model):
    with torch.no_grad():
        outputs = model(tokens_tensor, segments_tensors)
        hidden_states = outputs[2][1:]
    
    token_embeddings = hidden_states[-1]
    token_embeddings = torch.squeeze(token_embeddings, dim=0)
    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]

    return list_token_embeddings

target_word_embeddings1 = []
for text,word,ids in zip(texts1,words,id):
    tokenizer.add_tokens(word)
    tokenized_text, tokens_tensor, segments_tensors = berttweet_text_preparation(text, tokenizer)
    list_token_embeddings = get_berttweet_embeddings(tokens_tensor, segments_tensors, model)
    word_index = tokenized_text.index(word)
    word_embedding = list_token_embeddings[word_index]
    target_word_embeddings1.append(word_embedding)

len(target_word_embeddings1)

target_word_embeddings2 = []
for text,word in zip(texts2,words):
    tokenizer.add_tokens(word)
    tokenized_text, tokens_tensor, segments_tensors = berttweet_text_preparation(text, tokenizer)
    list_token_embeddings = get_berttweet_embeddings(tokens_tensor, segments_tensors, model)
    word_index = tokenized_text.index(word)
    word_embedding = list_token_embeddings[word_index]
    target_word_embeddings2.append(word_embedding)

len(target_word_embeddings2)

list_of_distances = []
for text1,text2,embed1,embed2 in zip(texts1,texts2,target_word_embeddings1,target_word_embeddings2):
  cos_dist = 1 - cosine(embed1, embed2)
  list_of_distances.append([text1, text2, cos_dist])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])
distances_df.head()

column_names=["target_word","y_train"]
tsv_data = pd.read_csv('/content/drive/MyDrive/TempoWiC_Starting_Kit/data/train.labels.tsv', sep='\t', names=column_names)
tsv_data.head()

fpr, tpr, thresholds = roc_curve(tsv_data['y_train'], distances_df['distance'])
f1_ls = []
for thres in thresholds:
    y_pred = np.where(distances_df['distance']>thres,1,0)
    f1_ls.append(f1_score(tsv_data['y_train'], y_pred, average='macro'))
    
f1_ls = pd.concat([pd.Series(thresholds), pd.Series(f1_ls)],axis=1)
f1_ls.columns = ['thresholds', 'f1']
f1_ls.sort_values(by='f1', ascending=False, inplace=True)
f1_ls.head()

thres=f1_ls.iloc[0].thresholds
thres

"""**VALIDATION**"""

val_instances = load_instances('/content/drive/MyDrive/TempoWiC_Starting_Kit/data/validation.data.jl')

texts1_val=[]
texts2_val=[]
words_val=[]
id_val=[]
for inst in val_instances:
  t1= " ".join(emoji.demojize(inst['tweet1']['text']).replace(","," ").replace(":"," ").replace("_"," ").split())
  t1=replace_contractions(t1)
  t1=remove_URL(t1)
  t1=remove_non_ascii(t1)
  t1=remove_punctuation(t1)
  t1=replace_numbers(t1)
  t1=to_lowercase(t1)
  texts1_val.append(t1)

  t2=" ".join(emoji.demojize(inst['tweet2']['text']).replace(","," ").replace(":"," ").replace("_"," ").split())
  t2=replace_contractions(t2)
  t2=remove_URL(t2)
  t2=remove_non_ascii(t2)
  t2=remove_punctuation(t2)
  t2=replace_numbers(t2)
  t2=to_lowercase(t2)
  texts2_val.append(t2)
  
  words_val.append(inst['word'])
  id_val.append(inst['id'])

model = AutoModel.from_pretrained("vinai/bertweet-base",output_hidden_states = True)
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base",use_fast=True)

for word in words_val:
  tokenizer.add_tokens(word)


model.resize_token_embeddings(len(tokenizer))

target_word_embeddings1 = []
target_word_embeddings2 = []
wordem=[]


for text,word,ids in zip(texts1_val,words_val,id_val):
    tokenizer.add_tokens(word)
    tokenized_text, tokens_tensor, segments_tensors = berttweet_text_preparation(text, tokenizer)
    list_token_embeddings = get_berttweet_embeddings(tokens_tensor, segments_tensors, model)
    word_index = tokenized_text.index(word)
    word_embedding = list_token_embeddings[word_index]
    target_word_embeddings1.append(word_embedding)

for text,word in zip(texts2_val,words_val):
    tokenizer.add_tokens(word)
    tokenized_text, tokens_tensor, segments_tensors = berttweet_text_preparation(text, tokenizer)
    list_token_embeddings = get_berttweet_embeddings(tokens_tensor, segments_tensors, model)
    word_index = tokenized_text.index(word)
    word_embedding = list_token_embeddings[word_index]
    target_word_embeddings2.append(word_embedding)

list_of_distances = []
for text1,text2,embed1,embed2 in zip(texts1_val,texts2_val,target_word_embeddings1,target_word_embeddings2):
  cos_dist = 1 - cosine(embed1, embed2)
  list_of_distances.append([text1, text2, cos_dist])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])

distances_df.head()

column_names=["target_word","y_train"]
tsv_data_val = pd.read_csv('/content/drive/MyDrive/TempoWiC_Starting_Kit/data/validation.labels.tsv', sep='\t', names=column_names)
tsv_data_val

y_pred_val = np.where(distances_df['distance']>thres,1,0)

conf_matrix = confusion_matrix(y_true=tsv_data_val['y_train'], y_pred=y_pred_val)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

print('F1 Score: %.3f' % f1_score(tsv_data_val['y_train'], y_pred_val,average='macro'))

print('Precision: %.3f' % precision_score(tsv_data_val['y_train'], y_pred_val))

print('Recall: %.3f' % recall_score(tsv_data_val['y_train'], y_pred_val))

print('Accuracy: %.3f' % accuracy_score(tsv_data_val['y_train'], y_pred_val))

"""**TEST**"""

test_instances = load_instances('/content/drive/MyDrive/TempoWiC_Starting_Kit/data/test-codalab-10k.data.jl')

texts1_test=[]
texts2_test=[]
words_test=[]
id_test=[]
for inst in test_instances:
  t1= " ".join(emoji.demojize(inst['tweet1']['text']).replace(","," ").replace(":"," ").replace("_"," ").split())
  t1=replace_contractions(t1)
  t1=remove_URL(t1)
  t1=remove_non_ascii(t1)
  t1=remove_punctuation(t1)
  t1=replace_numbers(t1)
  t1=to_lowercase(t1)
  t2=" ".join(emoji.demojize(inst['tweet2']['text']).replace(","," ").replace(":"," ").replace("_"," ").split())
  t2=replace_contractions(t2)
  t2=remove_URL(t2)
  t2=remove_non_ascii(t2)
  t2=remove_punctuation(t2)
  t2=replace_numbers(t2)
  t2=to_lowercase(t2)
  texts1_test.append(t1)
  texts2_test.append(t2)
  words_test.append(inst['word'])
  id_test.append(inst['id'])

model = AutoModel.from_pretrained("vinai/bertweet-base",output_hidden_states = True)
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base",use_fast=True)

for word in words_test:
  tokenizer.add_tokens(word)

model.resize_token_embeddings(len(tokenizer))

target_word_embeddings1_test = []
target_word_embeddings2_test = []


for text,word,ids in zip(texts1_test,words_test,id_test):
    tokenizer.add_tokens(word)
    tokenized_text, tokens_tensor, segments_tensors = berttweet_text_preparation(text, tokenizer)
    list_token_embeddings = get_berttweet_embeddings(tokens_tensor, segments_tensors, model)
    word_index = tokenized_text.index(word)
    word_embedding = list_token_embeddings[word_index]
    target_word_embeddings1_test.append(word_embedding)

for text,word in zip(texts2_test,words_test):
    tokenizer.add_tokens(word)
    tokenized_text, tokens_tensor, segments_tensors = berttweet_text_preparation(text, tokenizer)
    list_token_embeddings = get_berttweet_embeddings(tokens_tensor, segments_tensors, model)   
    word_index = tokenized_text.index(word)
    word_embedding = list_token_embeddings[word_index]
    target_word_embeddings2_test.append(word_embedding)

list_of_distances = []
for text1,text2,embed1,embed2 in zip(texts1_test,texts2_test,target_word_embeddings1_test,target_word_embeddings2_test):
  cos_dist = 1 - cosine(embed1, embed2)
  list_of_distances.append([text1, text2, cos_dist])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])
distances_df.head()

y_pred_test = np.where(distances_df['distance']>thres,1,0)

with open('/content/drive/MyDrive/TempoWiC_Starting_Kit Evaluation/TempoWiC_Starting_Kit/predictions/test.berttweet-preds.tsv', 'w') as preds_f:
  for inst_id, pred in zip(id_test,y_pred_test):
    preds_f.write(f"{inst_id}\t{pred}\n")